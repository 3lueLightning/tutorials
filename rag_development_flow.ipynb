{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu5SlSswE59tREHyPyrjVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3lueLightning/tutorials/blob/main/rag_development_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "This notebook aims to demonstrate the development process when starting to create RAG based LLM applications. The code is not optimized and goes through various development steps one at a time, including validations and handling of errors. Yes, you heard right—errors (but they are all caught in try-except clauses to prevent the code from breaking)."
      ],
      "metadata": {
        "id": "gCTSwCUZ_7A0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44rxBEt6DTcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984da3cd-331b-4725-f528-cc7ff71ed6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q langchain langchain_community tiktoken langchain_openai \\\n",
        "langchain_text_splitters docarray langsmith langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data\n",
        "Retrieval Augmented Generation (RAG) is essential for accessing information that the model hasn't learned, such as private data. Essentially, it involves providing data to the model through the message sent to the LLM (prompt). This often requires retrieving information relevant to the user's question from a database, which naturally involves loading this information into the database in the first place. In this example, we will be extracting a list of the top 100 best movies of all time according to Empire magazine and feeding it to a database. Let me know in the comments if you agree with the list! ;)"
      ],
      "metadata": {
        "id": "2xQL7kP1xeuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "MOVIES_URL = \"https://www.empireonline.com/movies/features/best-movies-2/\"\n",
        "\n",
        "# Note: Regarding ethical crawling, according to their robots.txt, all user\n",
        "# agents are allowed, and we are only accessing one page in the entire\n",
        "# notebook. Since all user agents are permitted, there's no need to fake one,\n",
        "# so you can disregard the warning bellow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBWDluNXKcj4",
        "outputId": "43458aa1-deb7-4824-e51c-980e4e514826"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test connection"
      ],
      "metadata": {
        "id": "ZNPhL84YiC7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we write the actual code to fetch web data for our LLM, we need to ensure that the page is accessible. Crawling can be complex, so it's prudent to verify our access with a simple HTTP request. WebBaseLoader does a combination of `request.get` and `bs4.BeautifulSoup`, more info [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/)"
      ],
      "metadata": {
        "id": "2TiPeuRvgi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the tutorial I add some exctract try-excepts to avoid issues. In real life\n",
        "# I'd test this line and if it succeeded I'd remove this entire cell.\n",
        "try:\n",
        "  full_page = WebBaseLoader(MOVIES_URL).load()\n",
        "except:\n",
        "  # I know it's not super clean to make an catch all exception\n",
        "  # but I rather be safe then sorry :P\n",
        "  full_page = []\n",
        "\n",
        "if not full_page:\n",
        "  print(\"The page didn't load, but no worries we have a backup ;)\")\n",
        "else:\n",
        "  print(full_page[0].page_content[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h974qYnP0Bb",
        "outputId": "0e8dab74-272e-4f96-bf2c-722300c18784"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 100 Best Movies Of All Time | Movies | %%chann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  print(full_page[0].page_content[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6HImSFBO6h3",
        "outputId": "74ee9694-8c5f-45ea-d75a-a8101c06cf78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 100 Best Movies Of All Time | Movies | %%chann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data"
      ],
      "metadata": {
        "id": "NZ6a4gr4iFjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the entire page's data but only a small fraction, so `SoupStainer` comes to the rescue. It allows us to extract information from HTML without loading everything into memory first and then filtering the results using `find_all`. This makes it the most efficient option. More information can be found [here](https://medium.com/codex/using-beautiful-soups-soupstrainer-to-save-time-and-memory-when-web-scraping-ea1dbd2e886f). So this time, we'll run the WebBasedLoader with the `bs_kwargs` argument to configure the strainer inside to extract only the information we need."
      ],
      "metadata": {
        "id": "qgAnn94mgIcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import SoupStrainer\n",
        "\n",
        "\n",
        "def is_target_element(elem: str, attrs: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Returns true if the HTML element is what we want to extract.\n",
        "    \"\"\"\n",
        "    # get the movie description\n",
        "    div_class = \"listicleItem_listicle-item__content__Lxn1Y\"\n",
        "    div_mask = (elem == \"div\" and attrs.get(\"class\") == div_class)\n",
        "    # get the movie title\n",
        "    h3_class = \"listicleItem_listicle-item__title__BfenH\"\n",
        "    h3_mask = (elem == \"h3\" and attrs.get(\"class\") == h3_class)\n",
        "    return div_mask or h3_mask\n",
        "\n",
        "strainer = SoupStrainer(is_target_element)\n",
        "\n",
        "\n",
        "movie_scraper = WebBaseLoader(\n",
        "    # Diogo: remove this\n",
        "    \"bbybu\",#MOVIES_URL,\n",
        "    bs_kwargs = {\n",
        "        \"parse_only\": strainer\n",
        "    }\n",
        ")\n",
        "\n",
        "# this try except is to prevent the code from crashing in case something\n",
        "# happens to the page\n",
        "try:\n",
        "  # only here is the page actually loaded\n",
        "  movie_reviews_raw = movie_scraper.load()\n",
        "except:\n",
        "  movie_reviews_raw = []"
      ],
      "metadata": {
        "id": "7VbgLZr_JOb-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  cell bellow is not really important, it simply downloads the page data from a backup system on AWS if the orginal page is not accessible for some reason. It only exists to the purpose of this tutorial and can be ignore if you're building your own LLM app"
      ],
      "metadata": {
        "id": "I7Bi6UZWGfuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (\n",
        "    not movie_reviews_raw\n",
        "    or movie_reviews_raw[0].page_content[:25] != \"100 Reservoir Dogs\\nMaking\"\n",
        "):\n",
        "  import pickle\n",
        "  import urllib\n",
        "  BACKUP_MOVIES_PKL_URL = \"https://tutorials-public.s3.eu-west-1.amazonaws.com/movie_reviews_raw.pkl\"\n",
        "  with urllib.request.urlopen(BACKUP_MOVIES_PKL_URL) as response:\n",
        "      movie_reviews_raw = pickle.load(response)\n",
        "  print(\"loaded list of top 10 best movies from backup system\")\n",
        "else:\n",
        "  print(\"successfully scrapped list of top 10 best movies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkbukTHlcdwz",
        "outputId": "c9456e7e-1392-4109-a774-586976633b20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded list of top 10 best movies from backup system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process data"
      ],
      "metadata": {
        "id": "cxYJEexOiNwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the movie reviews come as a single document, but we want to split them and remove the links to the full movie reviews at the end of each block"
      ],
      "metadata": {
        "id": "lBVaXXfDg24g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "def split_movies(page: Document) -> list[Document]:\n",
        "  page_parts = page.page_content.strip().split(\"\\n\")\n",
        "  names_n_reviews = [elem for elem in page_parts if not elem.startswith(\"Read\")]\n",
        "  movie_names = [re.sub(r'^\\d*\\)? ', \"\", name) for name in names_n_reviews[::2]]\n",
        "  movie_reviews = [\n",
        "      f\"{name}: {description}\"\n",
        "      for name, description in zip(movie_names, names_n_reviews[1::2])\n",
        "  ]\n",
        "  movie_docs = [\n",
        "      Document(review, metadata={**page.metadata, \"rank\": i, \"name\": name})\n",
        "      for review, i, name in zip(movie_reviews, range(100, 0, -1), movie_names)\n",
        "  ]\n",
        "  return movie_docs\n",
        "\n",
        "\n",
        "movie_reviews = split_movies(movie_reviews_raw[0])\n",
        "print(f\"extracted {len(movie_reviews)}\")\n",
        "movie_reviews[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhkDp7rtS5VN",
        "outputId": "2f4c3d82-1bb2-471d-ff77-e574d735b194"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 100, 'name': 'Reservoir Dogs'}, page_content=\"Reservoir Dogs: Making his uber cool and supremely confident directorial debut, Quentin Tarantino hit audiences with a terrific twist on the heist-gone-wrong thriller. For the most part a single location chamber piece, Reservoir Dogs delights in ricocheting the zing and fizz of its dialogue around its gloriously —and indeed gore-iously) — intense setting, with the majority of the movie's action centring around one long and incredibly bloody death scene. Packing killer lines, killer needledrops, and killer, er, killers too, not only is this a rollicking ride in its own right, but it also set the blueprint for everything we've come to expect from a Tarantino joint. Oh, and by the way: Nice Guy Eddie was shot by Mr. White. Who fired twice. Case closed.\")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis\n",
        "Let's check if there are any issues with the data and understand how much we will be charged by OpenAI. Keep in mind that we are charged based on tokens sent and received to and from their API. OpenAI's method of tokenizing is via \"tiktoken.\"\n"
      ],
      "metadata": {
        "id": "Jjk58lS1HqVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def count_tokens(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "token_counts = pd.Series(movie_reviews).apply(\n",
        "    lambda doc: count_tokens(doc.page_content, \"cl100k_base\")\n",
        ")\n",
        "token_counts.plot.hist(bins=20);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "iS7kL4QdiOqb",
        "outputId": "454ba610-d377-426d-a9cd-98c5085a0a4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that one movie has substantially more tokens than the others, so we need to ensure there are no issues. However, as you can see, the description for \"The Lord of the Rings\" is simply very lengthy, which, as any Tolkien fan would know, is only fair."
      ],
      "metadata": {
        "id": "LVf6-oaklWnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "movie_reviews[np.argmax(token_counts)].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "mAskiFTGlYoM",
        "outputId": "367d74b9-297d-4aec-9ca5-71c292a4def1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Lord Of The Rings: The Fellowship Of The Ring: A wizard is never late. Nor is he early. He arrives precisely when he... well, you know the rest. It might have taken 20 years for Peter Jackson's plucky fantasy to clamber, Mount-Doom-style, to the very pinnacle of our greatest-movies pantheon. But here it is, brighter and more resplendent than ever.The Fellowship Of The Ring contains so much movie. Even at the halfway point, as the characters take a breather to bicker in Rivendell, you already feel sated, like you've experienced more thrills, more suspense, more jollity and ethereal beauty than a regular film could possibly muster up. But Jackson is only getting started. Onwards his adventure hustles, to the bravura dungeoneering of Khazad-dum, to the sinisterly serene glades of Lothlorien, to the final requiem for flawed Boromir amidst autumnal leaves. As Fellowship thrums to its conclusion, finally applying the brakes with a last swell of Howard Shore's heavenly score, you're left feeling euphoric, bereft and hopeful, all at the same time. The Two Towers has the coolest battle. The Return Of The King boasts the most batshit, operatic spectacle. But Fellowship remains the most perfect of the three, matching every genius action beat with a soul-stirring emotional one, as its Middle-earth-traversing gang swells in size in the first act, then dwindles in the third. This oddball suicide squad has so much warmth and wit, they're not just believable as friends of each other — they've come to feel like they're our pals too.An ornately detailed masterwork with a huge, pulsing heart, it's just the right film for our times — full of craft, conviction and a belief that trudging forward, step by step, in dark days is the bravest act of all. Its ultimate heroes aren't the strongest, or those with the best one-liners, but the ones who just keep going. And so Fellowship endures: a miracle of storytelling, a feat of filmmaking and still the gold standard for cinematic experiences. Right, now that's decided, who's up for second breakfast?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up database\n",
        "## RAG in a nutshell (part 1)\n",
        "Our next goal is to make the various movie reviews accessible to our Large Language Model. Each time a user makes a query to the model, the system (here, Langchain) will:\n",
        "1. Convert the query into its numerical representation (embedding), capturing the essence of the query.\n",
        "2. Access an index (think of it as a database management system) to find the closest vector to our user's query vector (\"close\" often means highest \"cosine similarity\").\n",
        "3. Return the chunks corresponding to the k closest matches to the query (you, as the developer, define k).\n",
        "4. Pass the information to the model (more on this in part 2, further in this article).\n",
        "\n",
        "So the next steps are:\n",
        "1. Get the reference data - done (it's the movie data we scraped).\n",
        "2. Select an embedding function - essentially the method through which we will convert the text to a vector.\n",
        "3. Split the data into chunks - what we will insert into the vector database.\n",
        "4. Select a vector database - where we will store our chunks.\n",
        "5. Insert the data.\n",
        "6. Retrieve the data and feed it to the LLM - more on this later."
      ],
      "metadata": {
        "id": "UN3wmbgnitOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "Creating an embedding using Open API's models, more info [here](https://platform.openai.com/docs/guides/embeddings).\n",
        "\n",
        "Bellow you can set your OPENAI_API_KEY. If you don't have one you can create a legacy API KEY or a project API KEY where you can have more fine-grained access control, check [here](https://medium.com/@alozie_igbokwe/ai-api-key-essentials-part-1-how-to-set-up-your-openai-api-key-a-quick-beginner-guide-51c3a098b077)"
      ],
      "metadata": {
        "id": "ajXGvyomsU1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "OPENAI_API_KEY = getpass.getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv3yskBxn9rU",
        "outputId": "1f6b0958-98dd-4bd9-85ce-65f2cd0c74c6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-3-large\"\n",
        "\n",
        "embeder = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# testing embeding\n",
        "test_embedding = embeder.embed_query(\"What is 'Hello World'?\")\n",
        "print(test_embedding[:5])\n",
        "print(f\"the model {EMBEDDING_MODEL_NAME} generates embeddings\"\n",
        "      f\" of length: {len(test_embedding)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAzObdp5v1Wm",
        "outputId": "f785f921-7e03-43bb-c86a-99f432699f74"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.015843020752072334, -0.056327491998672485, -0.014403410255908966, 0.01967223733663559, -0.017874551936984062]\n",
            "the model text-embedding-3-large generates embeddings of length: 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insertion in Index\n",
        "By default `RecursiveCharacterTextSplitter` uses the following separators `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This means that it:\n",
        "1. first attempts to create a chunk (a piece of text) with as many paragraphs (separated by \"\\n\\n\") as possible without exceeding the character limit set by chunk_size.\n",
        "2. If it cannot even find one paragraph within the limit, it then tries to create chunks based on lines (\"\\n\").\n",
        "3. If it can't find a line within the constraint, it attempts to create chunks based on words (separated by spaces \" \").\n",
        "4. If none of these methods succeed, it resorts to match the character limit by individual characters.\n",
        "\n",
        "This recursive operation proceeds in the order above and stops as soon as one condition is met. When the first chunk is created then the corresponding text is removed and the process repeats itself for the rest of the string. If this explanation isn't clear, I encourage you to experiment with it yourself."
      ],
      "metadata": {
        "id": "TR1ruAFgi5Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# docarray was imported earlier to avoid an error when using from_documents()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "text_splitter.split_text(movie_reviews[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8X6JCW7e5kB",
        "outputId": "050c79e9-eca8-41d1-db82-0af8b612c471"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Donnie Darko: A high school drama with a time traveling, tangential universe threading, sinister rabbit featuring twist, Richard Kelly's deliberately labyrinthine opus was always destined for cult classic status. A certifiable flop upon its theatrical release, Kelly's film was one of the early beneficiaries of physical media's move to DVD, with the movie gaining a fandom in film obsessives who could pause, play, and skip back and forth through it at will. Any attempt to synopsise the movie is a fool's errand, but there's more than a hint of\\xa0It's A Wonderful Life in the way we see Donnie (Jake Gyllenhaal, in a star-making turn) experiencing how the world would be worse off if he survives the jet engine that mysteriously crashes through his bedroom. That the film, with all its heavy themes and brooding atmosphere, manages to eventually land on a note of overwhelming optimism is a testament to Kelly's mercurial moviemaking. A mad world (mad world) Donnie Darko's may be, but it's also one\",\n",
              " \"brooding atmosphere, manages to eventually land on a note of overwhelming optimism is a testament to Kelly's mercurial moviemaking. A mad world (mad world) Donnie Darko's may be, but it's also one that continues to beguile and fascinate as new fans find themselves obsessed with uncovering its mysteries.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=embeder,\n",
        "    text_splitter=text_splitter,\n",
        ").from_documents(movie_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7YXL_fvIWUr",
        "outputId": "04aebb7d-4fdc-4f7d-9c38-68c15e1c06ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Can you recommend me an adventure movie?\"\n",
        "\n",
        "retriever = index.vectorstore.as_retriever()\n",
        "relevant_movies = retriever.vectorstore.similarity_search(\n",
        "    QUESTION,\n",
        "    k=3 # by default k=4\n",
        ")\n",
        "for doc in relevant_movies:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEE6lvTv3XAO",
        "outputId": "91a4ba3c-84f3-42a1-bc2d-cce1a351510e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indiana Jones And The Last Crusade: You voted... wisely. There may only be 12 years' difference between Harrison Ford and Sean Connery, but it's hard to imagine two better actors to play a bickering father and son, off on a globetrotting, Nazi-bashing, mythical mystery tour. After all, you've got Spielberg/Lucas' own version of James Bond... And the original Bond himself.\n",
            "Raiders Of The Lost Ark: In '81, it must have sounded like the ultimate pitch: the creator of Star Wars teams up with the director of Jaws to make a rip-roaring, Bond-style adventure starring the guy who played Han Solo, in which the bad guys are the evillest ever (the Nazis) and the MacGuffin is a big, gold box which unleashes the power of God. It still sounds like the ultimate pitch.\n",
            "Lawrence Of Arabia: If you only ever see one David Lean movie... well, don't. Watch as many as you can. But if you really insist on only seeing one David Lean movie, then make sure it's Lawrence Of Arabia, the movie that put both the \"sweeping\" and the \"epic\" into \"sweeping epic\" with its breath-taking depiction of T.E. Lawrence's (Peter O'Toole) Arab-uniting efforts against the German-allied Turks during World War I. It's a different world to the one we're in now, of course, but Lean's mastery of expansive storytelling does much to smooth out any elements (such as Alec Guinness playing an Arab) that may rankle modern sensibilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the embeddings worked properly as we have succesfully extracted 3 adventure movies."
      ],
      "metadata": {
        "id": "B0C6hALIQ1kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "ppUG3Smplon3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "LLM_MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(\n",
        "    model=LLM_MODEL_NAME,\n",
        "    # higher temperature means more orginal answers so we set it to the max\n",
        "    temperature=1,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# testing that the LLM works\n",
        "llm.invoke(\"Hey how are you GPTie?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeksLIMMsRJa",
        "outputId": "e3cf6859-9918-41d4-e1b3-e670f49b5ab3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to assist you. How can I help you today?\", response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 15, 'total_tokens': 46}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-153ebd54-cf18-4662-822d-e104ba1306fd-0', usage_metadata={'input_tokens': 15, 'output_tokens': 31, 'total_tokens': 46})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use the prompt template created via LangChain Hub in [here](https://smith.langchain.com/hub/blue-lightning/movie-advisor). To create a prompt yourself you must log-in via LangSmith.\n",
        "\n",
        "Note that many other tutorials use `ChatPromptTemplate` instead, that is also fine, but it means that you have to store your templates in the code itself and it's not so easy to visualise or test. In addition it means you have to remember to git commit each time you tests a new prompt in order to be able to revert to it if needed. Finally it doesn't allow you to keep track of the performance of each promtp naturally - more on this in the next tutorial."
      ],
      "metadata": {
        "id": "9eq5UwAivnhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "\n",
        "chat_template = hub.pull(\"blue-lightning/movie-advisor\")\n",
        "chat_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvOlaoIYrfhM",
        "outputId": "bb18bcf9-6c72-495d-d0fb-41af99575d7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'input'], metadata={'lc_hub_owner': 'blue-lightning', 'lc_hub_repo': 'movie-advisor', 'lc_hub_commit_hash': 'c3f49564c55f02454221604292cc7fd1c8e1ba567f0e5d6719aa2b0892d5b1af'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='When asked a question reply as if you were the wizard of movies with the knowledge about movies. Try to be funny were possible but base you answers in the information provided in the context section.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='User question:\\n{input}\\n\\n-----------------------------------------\\nContext:\\n{context}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, chat_template)\n",
        "# you can print combine_docs_chain to the how the chain was built, it is in\n",
        "# LCEL (Lanchain expression language) which is beyond the scope of the tutorial"
      ],
      "metadata": {
        "id": "7e5lKl71498M"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "\n",
        "chat_chain = create_retrieval_chain(retriever, combine_docs_chain)"
      ],
      "metadata": {
        "id": "rxdY3wjhOx0n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # this is when your request actually goes to your LLM in this case the OpenAI servers\n",
        "  chat_answer = chat_chain.invoke({\"question\": QUESTION})\n",
        "except KeyError as e:\n",
        "  # gracefully handling the error. Note that the full trace was omitted because\n",
        "  # it's not very informative unless you know LCEL\n",
        "  print(f\"KeyError: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0SPnAXCqGi6",
        "outputId": "273ea9ff-51d1-440a-dd72-17f537485a6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-18-3cb4ab7ff88d>\", line 8, in <cell line: 7>\n",
            "    chat_answer = chat_chain.invoke({\"question\": QUESTION})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
            "    return self.bound.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
            "    input = step.invoke(input, config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
            "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 1599, in _call_with_config\n",
            "    context.run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
            "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\", line 456, in _invoke\n",
            "    **self.mapper.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 3152, in invoke\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 3152, in <dictcomp>\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
            "    return self.bound.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
            "    input = step.invoke(input, config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 3985, in invoke\n",
            "    return self._call_with_config(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 1599, in _call_with_config\n",
            "    context.run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
            "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 3853, in _invoke\n",
            "    output = call_func_with_variable_args(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
            "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\", line 61, in <lambda>\n",
            "    retrieval_docs = (lambda x: x[\"input\"]) | retriever\n",
            "KeyError: 'input'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error was caused on purpose to hammer home that the variables in your prompt must be:\n",
        "1. `context`: to provide the results of the retriever in `create_stuff_documents_chain`\n",
        "2. `input`: to provide the user question in  `create_retrieval_chain`"
      ],
      "metadata": {
        "id": "ppj_LD6bzFTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "# you can replace ConsoleCallbackHandler by FileCallbackHandler if you wish to\n",
        "# save the trace to file. A better option still is to save it to LangSmith\n",
        "# but check the next tutorial for more on this ;)\n",
        "\n",
        "try:\n",
        "  chat_answer = chat_chain.invoke(\n",
        "      {\"question\": QUESTION},\n",
        "      config={'callbacks': [ConsoleCallbackHandler()]}\n",
        "  )\n",
        "except KeyError as e:\n",
        "  # gracefully handling the error. Note that the full trace was omitted because\n",
        "  # it's not very informative unless you know LCEL\n",
        "  print(f\"KeyError: {e}\")\n",
        "\n",
        "# the code bellow is equivalent to the above but gives you less control\n",
        "# Plus as Uncle Bob once said (and I paraphrase): \"people are terrible at\n",
        "# doing things that come in pairs\", so it is likely that any of us will forget\n",
        "# to revert the debug to False. This is most useful if you want to debug every chain\n",
        "#\n",
        "# import langchain\n",
        "#\n",
        "# langchain.debug = True\n",
        "# chat_answer = chat_chain.invoke({\"input\": QUESTION})\n",
        "# langchain.debug = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqichxrc6i7t",
        "outputId": "f18b8c74-dcfb-411e-a6f3-00adc437ce4c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] [2ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3853, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] [4ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2505, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3985, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3853, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [9ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4588, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2505, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3985, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3853, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] [16ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4588, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2505, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3985, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3853, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain] [25ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2505, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 469, in invoke\\n    return self._call_with_config(self._invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3152, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4588, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2505, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3985, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1599, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3853, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "KeyError: 'input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "\n",
        "adventure_movies = chat_chain.invoke({\"input\": QUESTION})\n",
        "# we use pprint rather then simply print to have all the text fit the screen\n",
        "pprint(adventure_movies[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H73dm7w7rxvm",
        "outputId": "ecce4837-960f-4fc5-a1f1-b9e00dc85d8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('As the wizard of movies, I shall grant you the recommendation you seek! For '\n",
            " 'a thrilling adventure movie, I bestow upon you the epic journey of \"The Lord '\n",
            " 'of the Rings\" trilogy. Watch as Frodo Baggins embarks on a perilous quest to '\n",
            " 'destroy the One Ring and save Middle-earth from the clutches of the dark '\n",
            " 'lord Sauron. Filled with breathtaking landscapes, heart-pounding battles, '\n",
            " 'and memorable characters, this adventure will have you on the edge of your '\n",
            " 'seat from start to finish. So, grab your sword, summon your courage, and '\n",
            " 'prepare for an unforgettable cinematic adventure!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try with another more complicated question who's answer requires a bit more thinking. Yet after a quick inspection of each movie we can see it is indeed a surrealist movie. Oviously this answers are not deterministic and you can have different ones, but I obtained: Pan's Labyrinth, Amelie, Vertigo and Mulholland Drive."
      ],
      "metadata": {
        "id": "ctoz7xpdbc6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surrealist_movies = chat_chain.invoke({\"input\": \"Which surrealist movies should I watch ?\"})\n",
        "for key, val in surrealist_movies.items():\n",
        "  print(10 * \"-\" + f\" {key} \" + 10 * \"-\")\n",
        "  pprint(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtC850qyiBDx",
        "outputId": "87cb6780-be5a-46ce-c42a-a4667c673227"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- input ----------\n",
            "'Which surrealist movies should I watch ?'\n",
            "---------- context ----------\n",
            "[Document(page_content=\"Pan's Labyrinth: Guillermo Del Toro's fairy tale for grown-ups, as pull-no-punches brutal as it is gorgeously, baroquely fantastical. There's an earthy, primal feel to his fairy-world here, alien and threatening rather than gasp-inducing and 'magical', thanks in no small part to the truly cheese-dream nightmarish demon-things Del Toro conjures up, sans CGI, with the assistance of performer Doug Jones.\", metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 61, 'name': \"Pan's Labyrinth\"}),\n",
            " Document(page_content=\"Amelie: Jean-Pierre Jeunet's fourth feature – his second as a solo artist divorced from Marc Caro – saw the\\xa0Delicatessen,\\xa0The City of Lost Children\\xa0and\\xa0Alien: Resurrection filmmaker leave behind the overwhelming darkness of his earlier works and step out into the glorious sunshine of Amelie's whimsical fantasy Paris. Sure, a cynic could read the film as the story of Audrey Tatou's monomaniacal title character's relentless, somewhat stalkerish pursuit of the hapless Nino (Matthieu Kassovitz) around Montmartre's dream-like cityscape. But this one isn't for the cynics — it's a tribute to the daydreamers of this world. It's a sweet, nostalgic, sentimental, beautifully sunny, and unforcedly quirky romantic comedy played out amidst a veritable visual fantasia that only Jeunet could have conceived. Amelie will always be on our list of things we like.\", metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 97, 'name': 'Amelie'}),\n",
            " Document(page_content=\"Vertigo: If Psycho was Hitchcock's big shocker, then Vertigo is the one that gets properly under your skin. With James Stewart's detective stalking Kim Novak's mysterious woman, witnessing her suicide, then becoming obsessed with her double, it's certainly disturbing and most definitely (as the title suggests) disorientating. In the most artful and inventive way.\", metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 50, 'name': 'Vertigo'}),\n",
            " Document(page_content=\"Mulholland Drive: David Lynch messes with Hollywood itself in a mystery tale that's as twisted as the road it's named after, while presenting Tinseltown as both Dream Factory and a realm of Nightmares. It also put Naomi Watts on the map; her audition scene remains as stunning as it was 20 years ago.\", metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 73, 'name': 'Mulholland Drive'})]\n",
            "---------- answer ----------\n",
            "('Ah, dear movie enthusiast, if you seek to delve into the enigmatic world of '\n",
            " 'surrealist cinema, I have just the wizardly recommendations for you!\\n'\n",
            " '\\n'\n",
            " \"First, venture into Guillermo Del Toro's twisted fairy tale masterpiece, \"\n",
            " '\"Pan\\'s Labyrinth.\" This film will transport you to a fantastical yet brutal '\n",
            " 'world where demons lurk and nightmares come alive, all without the need for '\n",
            " 'CGI trickery – just pure cinematic magic.\\n'\n",
            " '\\n'\n",
            " 'Next, lose yourself in the whimsical charm of \"Amelie,\" crafted by the '\n",
            " 'visionary Jean-Pierre Jeunet. Follow the quirky adventures of the titular '\n",
            " 'character as she navigates a dream-like Paris in pursuit of love and whimsy. '\n",
            " \"It's a romantic comedy like no other, perfect for the daydreamers at heart.\\n\"\n",
            " '\\n'\n",
            " 'For a mind-bending Hitchcockian experience, set your eyes on \"Vertigo.\" With '\n",
            " 'its haunting tale of obsession and illusion, this film will send shivers '\n",
            " 'down your spine and leave you questioning reality in the most artful way '\n",
            " 'possible.\\n'\n",
            " '\\n'\n",
            " 'And finally, prepare to enter the surreal, twisted world of David Lynch with '\n",
            " '\"Mulholland Drive.\" This mystery film will take you on a dark and twisted '\n",
            " \"journey through Hollywood's underbelly, where dreams and nightmares \"\n",
            " 'intertwine in the most mesmerizing and bizarre manner.\\n'\n",
            " '\\n'\n",
            " 'So grab your popcorn, brace yourself for the unexpected, and embark on a '\n",
            " 'cinematic adventure unlike any other with these surrealist gems!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the 2nd question, we displayed not only the answer as previously but also the input (original question) and context (the result from the RAG). This allows us to verify that indeed that right question was fed to the model and that the context retreived ffrom our vector DB is not only relevant to the question but also that the answer is indeed rooted in the provided context and the model didn't hallucinate.\n",
        "\n",
        "This is obviously a non-scalable manual validation but it is contintutes the first step in our testing / validation of our software (ETL + database retrieval + LLM). In the next article we will see how to automatite it and scale it. Stay tuned for more"
      ],
      "metadata": {
        "id": "eTb7yrEd4M1M"
      }
    }
  ]
}