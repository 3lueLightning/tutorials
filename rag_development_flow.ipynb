{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpRDl2QIG/c4yW66MfOo34",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3lueLightning/tutorials/blob/main/rag_development_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "\n",
        "This notebook aims to demonstrate the development process when starting to create RAG based LLM applications. The code is not optimized and goes through various development steps one at a time, including validations and handling of errors. Yes, you heard rightâ€”errors (but they are all caught in try-except clauses to prevent the code from breaking)."
      ],
      "metadata": {
        "id": "gCTSwCUZ_7A0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44rxBEt6DTcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d6c39b-276a-4b27-a7af-3dbb7074817b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q langchain langchain_community tiktoken langchain_openai \\\n",
        "langchain_text_splitters docarray langsmith langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading the data\n",
        "LLMs excel at answering questions and powering chatbots, but their knowledge is limited to their training data. RAG is crucial for accessing new information, such as private data, by providing relevant data through prompts. This often involves retrieving information from a database (DB) and loading it initially. In this example, we'll extract Empire magazine's top 100 movies of all time and feed it into a database. Let me know in the comments if you agree with the list! ðŸ˜‰"
      ],
      "metadata": {
        "id": "2xQL7kP1xeuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "MOVIES_URL = \"https://www.empireonline.com/movies/features/best-movies-2/\"\n",
        "\n",
        "# Note regarding ethical crawling: according to their robots.txt, all user\n",
        "# agents are allowed, and we are only accessing one page in the entire\n",
        "# notebook. Since all user agents are permitted, there's no need to fake one,\n",
        "# so you can disregard the warning below."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBWDluNXKcj4",
        "outputId": "87732a57-6dd9-4c5f-9679-a6d42a3c4b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Test connection"
      ],
      "metadata": {
        "id": "ZNPhL84YiC7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we write the actual code to fetch web data for our LLM, we need to ensure that the page is accessible. Crawling can be complex, so it's prudent to verify our access with a simple HTTP request. WebBaseLoader does a combination of `requests.get` and `bs4.BeautifulSoup`, more info [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/)"
      ],
      "metadata": {
        "id": "2TiPeuRvgi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the tutorial I added some extra try-excepts to avoid issues. In real life\n",
        "# I'd test this line and if it succeeded I'd remove this entire cell.\n",
        "try:\n",
        "  full_page = WebBaseLoader(MOVIES_URL).load()\n",
        "  print(full_page[0].page_content[:50])\n",
        "except:\n",
        "  # I know it's not super clean to make an catch all exception\n",
        "  # but I rather be safe then sorry :P\n",
        "  full_page = []\n",
        "  print(\"The page didn't load, but no worries we have a backup ;)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h974qYnP0Bb",
        "outputId": "6ad8a9d2-0a5b-4fe2-e44d-92a5369fd3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 100 Best Movies Of All Time | Movies | %%chann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Get data"
      ],
      "metadata": {
        "id": "NZ6a4gr4iFjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the entire page's data but only a small fraction, so `SoupStainer` comes to the rescue. It allows us to extract information from HTML without loading everything into memory first and then filtering the results using `find_all`. This makes it the most efficient option. More information can be found [here](https://medium.com/codex/using-beautiful-soups-soupstrainer-to-save-time-and-memory-when-web-scraping-ea1dbd2e886f). So this time, we'll run the WebBasedLoader with the `bs_kwargs` argument to configure the strainer inside to extract only the information we need."
      ],
      "metadata": {
        "id": "qgAnn94mgIcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import SoupStrainer\n",
        "\n",
        "\n",
        "def is_target_element(elem: str, attrs: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Returns true if the HTML element is what we want to extract.\n",
        "    \"\"\"\n",
        "    # get the movie description\n",
        "    div_class = \"listicleItem_listicle-item__content__Lxn1Y\"\n",
        "    div_mask = (elem == \"div\" and attrs.get(\"class\") == div_class)\n",
        "    # get the movie title\n",
        "    h3_class = \"listicleItem_listicle-item__title__BfenH\"\n",
        "    h3_mask = (elem == \"h3\" and attrs.get(\"class\") == h3_class)\n",
        "    return div_mask or h3_mask\n",
        "\n",
        "strainer = SoupStrainer(is_target_element)\n",
        "\n",
        "\n",
        "movie_scraper = WebBaseLoader(\n",
        "    MOVIES_URL,\n",
        "    bs_kwargs = {\n",
        "        \"parse_only\": strainer\n",
        "    }\n",
        ")\n",
        "\n",
        "# this try except is to prevent the code from crashing in case something\n",
        "# happens to the page\n",
        "try:\n",
        "  # only here is the page actually loaded\n",
        "  movie_reviews_raw = movie_scraper.load()\n",
        "except:\n",
        "  movie_reviews_raw = []"
      ],
      "metadata": {
        "id": "7VbgLZr_JOb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  cell below is not really important, it simply downloads the page data from a backup system on AWS if the orginal page is not accessible for some reason. It only exists to the purpose of this tutorial and can be ignore if you're building your own LLM app"
      ],
      "metadata": {
        "id": "I7Bi6UZWGfuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no need to read this cell it is just a backup system in case the scraping fails\n",
        "if (\n",
        "    not movie_reviews_raw\n",
        "    or movie_reviews_raw[0].page_content[:25] != \"100 Reservoir Dogs\\nMaking\"\n",
        "):\n",
        "  import pickle\n",
        "  import urllib\n",
        "  BACKUP_MOVIES_PKL_URL = \"https://tutorials-public.s3.eu-west-1.amazonaws.com/movie_reviews_raw.pkl\"\n",
        "  with urllib.request.urlopen(BACKUP_MOVIES_PKL_URL) as response:\n",
        "      movie_reviews_raw = pickle.load(response)\n",
        "  print(\"loaded list of top 10 best movies from backup system\")\n",
        "else:\n",
        "  print(\"successfully scrapped list of top 10 best movies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkbukTHlcdwz",
        "outputId": "8608cfba-7d99-4497-ef08-f44dfb7cd019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully scrapped list of top 10 best movies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Process data"
      ],
      "metadata": {
        "id": "cxYJEexOiNwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the movie reviews come as a single document, but we want to split them and remove the links to the full movie reviews at the end of each block"
      ],
      "metadata": {
        "id": "lBVaXXfDg24g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "def split_movies(page: Document) -> list[Document]:\n",
        "  \"\"\"\n",
        "  Split page into a list of movie reviews\n",
        "  \"\"\"\n",
        "  page_parts = page.page_content.strip().split(\"\\n\")\n",
        "  names_n_reviews = [p for p in page_parts if not p.startswith(\"Read\")]\n",
        "  pattern = r'^\\d*\\)? '\n",
        "  movie_names = [re.sub(pattern, \"\", name) for name in names_n_reviews[::2]]\n",
        "  movie_reviews = [\n",
        "      f\"{name}: {description}\"\n",
        "      for name, description in zip(movie_names, names_n_reviews[1::2])\n",
        "  ]\n",
        "  movie_docs = [\n",
        "      Document(review, metadata={**page.metadata, \"rank\": i, \"name\": name})\n",
        "      for review, i, name in zip(movie_reviews, range(100, 0, -1), movie_names)\n",
        "  ]\n",
        "  return movie_docs\n",
        "\n",
        "\n",
        "movie_reviews = split_movies(movie_reviews_raw[0])\n",
        "print(f\"extracted {len(movie_reviews)}\")\n",
        "movie_reviews[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhkDp7rtS5VN",
        "outputId": "9a63329b-d907-4beb-b74a-ca409653613a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 100, 'name': 'Reservoir Dogs'}, page_content=\"Reservoir Dogs: Making his uber cool and supremely confident directorial debut, Quentin Tarantino hit audiences with a terrific twist on the heist-gone-wrong thriller. For the most part a single location chamber piece, Reservoir Dogs delights in ricocheting the zing and fizz of its dialogue around its gloriously â€”and indeed gore-iously) â€” intense setting, with the majority of the movie's action centring around one long and incredibly bloody death scene. Packing killer lines, killer needledrops, and killer, er, killers too, not only is this a rollicking ride in its own right, but it also set the blueprint for everything we've come to expect from a Tarantino joint. Oh, and by the way: Nice Guy Eddie was shot by Mr. White. Who fired twice. Case closed.\")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`OPENAI_API_KEY`"
      ],
      "metadata": {
        "id": "shEV6Paklh0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Analysis\n",
        "(this mini section isn't present on the Medium article)  \n",
        "Let's check if there are any issues with the data and understand how much we will be charged by OpenAI. Keep in mind that we are charged based on tokens sent and received to and from their API. OpenAI's method of tokenizing is via \"tiktoken.\"\n"
      ],
      "metadata": {
        "id": "Jjk58lS1HqVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def count_tokens(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "token_counts = pd.Series(movie_reviews).apply(\n",
        "    lambda doc: count_tokens(doc.page_content, \"cl100k_base\")\n",
        ")\n",
        "token_counts.plot.hist(bins=20);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "iS7kL4QdiOqb",
        "outputId": "d17db8a7-9337-489f-c2ee-ee477f9cf648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that one movie has substantially more tokens than the others, so we need to ensure there are no issues. However, as you can see, the description for \"The Lord of the Rings\" is simply very lengthy, which, as any Tolkien fan would know, is only fair."
      ],
      "metadata": {
        "id": "LVf6-oaklWnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "movie_reviews[np.argmax(token_counts)].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "mAskiFTGlYoM",
        "outputId": "26d56e41-88b1-4ef2-84f2-6b3328709ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Lord Of The Rings: The Fellowship Of The Ring: A wizard is never late. Nor is he early. He arrives precisely when he... well, you know the rest. It might have taken 20 years for Peter Jackson's plucky fantasy to clamber, Mount-Doom-style, to the very pinnacle of our greatest-movies pantheon. But here it is, brighter and more resplendent than ever.The Fellowship Of The Ring contains so much movie. Even at the halfway point, as the characters take a breather to bicker in Rivendell, you already feel sated, like you've experienced more thrills, more suspense, more jollity and ethereal beauty than a regular film could possibly muster up. But Jackson is only getting started. Onwards his adventure hustles, to the bravura dungeoneering of Khazad-dum, to the sinisterly serene glades of Lothlorien, to the final requiem for flawed Boromir amidst autumnal leaves. As Fellowship thrums to its conclusion, finally applying the brakes with a last swell of Howard Shore's heavenly score, you're left feeling euphoric, bereft and hopeful, all at the same time. The Two Towers has the coolest battle. The Return Of The King boasts the most batshit, operatic spectacle. But Fellowship remains the most perfect of the three, matching every genius action beat with a soul-stirring emotional one, as its Middle-earth-traversing gang swells in size in the first act, then dwindles in the third. This oddball suicide squad has so much warmth and wit, they're not just believable as friends of each other â€” they've come to feel like they're our pals too.An ornately detailed masterwork with a huge, pulsing heart, it's just the right film for our times â€” full of craft, conviction and a belief that trudging forward, step by step, in dark days is the bravest act of all. Its ultimate heroes aren't the strongest, or those with the best one-liners, but the ones who just keep going. And so Fellowship endures: a miracle of storytelling, a feat of filmmaking and still the gold standard for cinematic experiences. Right, now that's decided, who's up for second breakfast?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Set up database\n",
        "## 3.1 RAG in a nutshell - part 1\n",
        "Our next goal is to make various movie reviews accessible to our Large Language Model. The steps to achieve this are:\n",
        "1. **Embedding Selection**: Convert text into its numerical representation (embedding) to capture its essence. Similar texts will have similar vectors (e.g., the embedding for \"man\" is similar to that for \"woman\" and very different from \"meteor\").\n",
        "2. **Chunking**: Split the original text (movie reviews) into smaller pieces before inserting them into the database.\n",
        "3. **Database Creation and Insertion**: Insert the chunks and their corresponding embeddings into a database.\n",
        "4. **Data Retrieval**: After embedding the user query, find the closest vector to the query vector (using cosine similarity). Return the chunks corresponding to the k closest matches (you define k).\n",
        "\n",
        "Then pass the retrieved information to the LLM (more details in part 2 of this article)."
      ],
      "metadata": {
        "id": "UN3wmbgnitOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Embedding selection\n",
        "To convert text into embeddings we use Open API's models, more info [here](https://platform.openai.com/docs/guides/embeddings).\n",
        "\n",
        "Below you can set your OPENAI_API_KEY, this is safe way of handling it. If you don't have one you can create a legacy API KEY or a project API KEY where you can have more fine-grained access control, check [here](https://medium.com/@alozie_igbokwe/ai-api-key-essentials-part-1-how-to-set-up-your-openai-api-key-a-quick-beginner-guide-51c3a098b077). With the basic [free tier](https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-free) you can use all the code in this notebook without concern."
      ],
      "metadata": {
        "id": "ajXGvyomsU1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "OPENAI_API_KEY = getpass.getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv3yskBxn9rU",
        "outputId": "546b3c51-e551-4150-cc34-5ff246c9a31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# OpenAI has multiple models, transforms the text in longer vectors (here\n",
        "# length of 3072) and carries out more information about the original text.\n",
        "# It is also more expensive and requires more space to store.\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-3-large\"\n",
        "\n",
        "embeder = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# testing embeding\n",
        "test_embedding = embeder.embed_query(\"What is 'Hello World'?\")\n",
        "print(test_embedding[:5])\n",
        "print(f\"the model {EMBEDDING_MODEL_NAME} generates embeddings\"\n",
        "      f\" of length: {len(test_embedding)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAzObdp5v1Wm",
        "outputId": "b6ce2ef0-79cb-4b8e-fab3-efb50ced45f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.015853295102715492, -0.056399740278720856, -0.014421384781599045, 0.019666852429509163, -0.017855048179626465]\n",
            "the model text-embedding-3-large generates embeddings of length: 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Chunking\n",
        "By default `RecursiveCharacterTextSplitter` uses the following separators `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This means that it:\n",
        "1. It first tries to create chunks with as many paragraphs as possible without exceeding the chunk_sizelimit (using \"\\n\\n\" as a separator).\n",
        "2. If a paragraph exceeds the limit, it then splits based on lines (\"\\n\").\n",
        "3. If a line exceeds the limit, it splits based on words (\" \").\n",
        "4. If a word exceeds the limit, it splits by individual characters.\n",
        "\n",
        "This recursive process stops once a condition is met, removing the chunked text and repeating for the rest of the string."
      ],
      "metadata": {
        "id": "TR1ruAFgi5Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# docarray was imported earlier to avoid an error when using from_documents()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "text_splitter.split_text(movie_reviews[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8X6JCW7e5kB",
        "outputId": "4533b048-246e-41a8-e5b5-6cec906a4680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Donnie Darko: A high school drama with a time traveling, tangential universe threading, sinister rabbit featuring twist, Richard Kelly's deliberately labyrinthine opus was always destined for cult classic status. A certifiable flop upon its theatrical release, Kelly's film was one of the early beneficiaries of physical media's move to DVD, with the movie gaining a fandom in film obsessives who could pause, play, and skip back and forth through it at will. Any attempt to synopsise the movie is a fool's errand, but there's more than a hint of\\xa0It's A Wonderful Life in the way we see Donnie (Jake Gyllenhaal, in a star-making turn) experiencing how the world would be worse off if he survives the jet engine that mysteriously crashes through his bedroom. That the film, with all its heavy themes and brooding atmosphere, manages to eventually land on a note of overwhelming optimism is a testament to Kelly's mercurial moviemaking. A mad world (mad world) Donnie Darko's may be, but it's also one\",\n",
              " \"brooding atmosphere, manages to eventually land on a note of overwhelming optimism is a testament to Kelly's mercurial moviemaking. A mad world (mad world) Donnie Darko's may be, but it's also one that continues to beguile and fascinate as new fans find themselves obsessed with uncovering its mysteries.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Database: creation and insertion\n",
        "The index helps us manage our vector database, so named because data is retrieved by comparing the vector (embedding) of the query to the vectors (embeddings) of the chunks in the database. While there are many vector databases available, we'll keep it simple for this example. Given the small dataset, we can store all the data in memory using `DocArrayInMemorySearch`.\n"
      ],
      "metadata": {
        "id": "m74WXIgiRaoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "\n",
        "# from_documents is the method that inserts or list of documents in the DB\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=embeder,\n",
        "    text_splitter=text_splitter,\n",
        ").from_documents(movie_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7YXL_fvIWUr",
        "outputId": "57ae47ad-f70e-41c5-b255-e4159fe7bd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Data Retrieval\n",
        "Test that the index is working. As you can see the embeddings and index worked properly as we have succesfully extracted 3 adventure movies."
      ],
      "metadata": {
        "id": "n9Nyp7tATcsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is purely to test everything worked as planned\n",
        "\n",
        "retriever = index.vectorstore.as_retriever()\n",
        "# find the closest matches to the query\n",
        "relevant_movies = retriever.vectorstore.similarity_search(\n",
        "    \"Can you recommend me an adventure movie?\",\n",
        "    k=3 # by default k=4\n",
        ")\n",
        "for doc in relevant_movies:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEE6lvTv3XAO",
        "outputId": "7adc92ac-ae22-4e75-e221-77a994af476d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indiana Jones And The Last Crusade: You voted... wisely. There may only be 12 years' difference between Harrison Ford and Sean Connery, but it's hard to imagine two better actors to play a bickering father and son, off on a globetrotting, Nazi-bashing, mythical mystery tour. After all, you've got Spielberg/Lucas' own version of James Bond... And the original Bond himself.\n",
            "Raiders Of The Lost Ark: In '81, it must have sounded like the ultimate pitch: the creator of Star Wars teams up with the director of Jaws to make a rip-roaring, Bond-style adventure starring the guy who played Han Solo, in which the bad guys are the evillest ever (the Nazis) and the MacGuffin is a big, gold box which unleashes the power of God. It still sounds like the ultimate pitch.\n",
            "Lawrence Of Arabia: If you only ever see one David Lean movie... well, don't. Watch as many as you can. But if you really insist on only seeing one David Lean movie, then make sure it's Lawrence Of Arabia, the movie that put both the \"sweeping\" and the \"epic\" into \"sweeping epic\" with its breath-taking depiction of T.E. Lawrence's (Peter O'Toole) Arab-uniting efforts against the German-allied Turks during World War I. It's a different world to the one we're in now, of course, but Lean's mastery of expansive storytelling does much to smooth out any elements (such as Alec Guinness playing an Arab) that may rankle modern sensibilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create RAG chain\n",
        "## 4.1. RAG in a Nutshell - Part 2\n",
        "Having successfully set up an in-memory vector database (managed by an index) populated with movie review data, our next steps are to:\n",
        "1. Set up the LLM: This involves selecting a model and establishing an API connection.\n",
        "2. Create the prompt template: the code sort of \"meta prompt\" that wraps the user's query.\n",
        "3. Create the RAG chain: This chain automatically retrieves data from the database based on the user query, inserts the retrieved content into the \"meta-prompt\" sent to the LLM, and includes the actual user query/prompt.\n",
        "4. Obtain the final answer.\n",
        "\n",
        "## 4.2. Setting Up the LLM\n",
        "We will use OpenAI's GPT-3.5 Turbo. This model is part of the free tier with a limit of 3 RPM (requests per minute) at the time of writing. For more advanced models like GPT-4, you'll need to spend at least $5 to move to Tier 1 (the lowest of five tiers).\n",
        "\n",
        "There are many other model providers, such as Anthropic, Mistral AI, and others, constantly vying for supremacy. Additionally, there are open-source models like Llama3 that can be hosted locally on your laptop using tools like Ollama, or via the cloud with Amazon BedRock and others. I chose OpenAI for this first article because of its simplicity, but in future articles, I will explore other options.\n",
        "\n",
        "This brings me to one of the reasons to use the LangChain framework rather than OpenAI directly: it prevents vendor and model lock-in. LangChain allows you to switch models and vendors with minimal code changes. While some adjustments, such as optimizing your prompt for the model you are using, are always needed, LangChain substantially reduces the effort involved."
      ],
      "metadata": {
        "id": "ppUG3Smplon3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will use the API key set up above\n",
        "# note: we are note using the OpenAI API directly but using it via langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "LLM_MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(\n",
        "    model=LLM_MODEL_NAME,\n",
        "    # higher temperature means more original answers\n",
        "    temperature=1,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# testing that the LLM works, you will observe that the final answer provides\n",
        "# a count of the tokens used in the prompt and in the reply (completion_tokens)\n",
        "# Which are what you will be charged for outside of the free tier\n",
        "llm.invoke(\"Hey how are you GPTie?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeksLIMMsRJa",
        "outputId": "8a55ff05-44ba-4d02-88c5-8a6f2c78bd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with any questions or needs you have. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 15, 'total_tokens': 55}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3983ca98-c1dd-4035-8e7c-dbf897e1355e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 40, 'total_tokens': 55})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Create the prompt template\n",
        "The main reason to create a prompt template rather then an f-string it integrate it with the rest of LangChain and to more clearly estrablish the various roles in the mechanism (eg. \"system\", \"human\", etc.). Here we ask the LLM to provide movie recommendations as if it was movie wizard."
      ],
      "metadata": {
        "id": "yOvHlxPXN3bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# define how the LLM should respond in general\n",
        "system_message = \"\"\"\n",
        "When asked a question reply as if you were the wizard of movies with the \\\n",
        "knowledge about movies. Try to be funny were possible but base you answers in \\\n",
        "the information provided in the context section.\\\n",
        "\"\"\"\n",
        "# wraps the user query represented by {question} and provides documents from\n",
        "# the vector DB in {context}\n",
        "human_message = \"\"\"\n",
        "User question:\n",
        "{question}\n",
        "-----------------------------------------\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "# put all the messages together into a single prompt\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_message),\n",
        "    (\"human\", human_message),\n",
        "])"
      ],
      "metadata": {
        "id": "jvOlaoIYrfhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Create the RAG chain\n",
        "A LangChain chain is a sequence of modular components (like prompts, models, or tools) that are linked together to process inputs and generate desired outputs in a structured manner. We join the following pieces into one:\n",
        "1. Prompt template\n",
        "2. LLM\n",
        "3. Approach for integrating the retrieved documents from the database\n",
        "\n",
        "While the first two elements are straightforward, the third might be a bit more obscure. Indeed, if we retrieve five documents from the vector database, how do we combine them into a single block of text to insert in `{context}`? The most obvious way is to stuff them all together, meaning appending the `page_content` of each document one after the other, which is what `create_stuff_documents_chain` does. However, there are many other methods, for example for very large documents each document can be summarizing before the summaries are stuffed together."
      ],
      "metadata": {
        "id": "YzqLCCrg059Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, chat_template)\n",
        "# you can print combine_docs_chain to the how the chain was built, it is in\n",
        "# LCEL (Lanchain expression language) which is beyond the scope of the tutorial"
      ],
      "metadata": {
        "id": "7e5lKl71498M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next chain adds a retriver so that our RAG can access the movie data"
      ],
      "metadata": {
        "id": "siVe_-YuPCHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "\n",
        "chat_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "# print chat_chain to view the LCEL code behind it"
      ],
      "metadata": {
        "id": "rxdY3wjhOx0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Can you recommend me an adventure movie?\"\n",
        "try:\n",
        "  # it is only now that your request goes to the LLM\n",
        "  # the dictionary key represents what you want to replace in the template\n",
        "  chat_answer = chat_chain.invoke({\"question\": QUESTION})\n",
        "except KeyError as e:\n",
        "  # gracefully handling the error. Note that the full trace was omitted because\n",
        "  # it's not very informative unless you know LCEL\n",
        "  print(f\"KeyError: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0SPnAXCqGi6",
        "outputId": "9c909910-04d4-472d-afe9-a88584ef59b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KeyError: 'input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "# you can replace ConsoleCallbackHandler by FileCallbackHandler if you wish to\n",
        "# save the trace to file. A better option still is to save it to LangSmith\n",
        "# but check the next tutorial for more on this ;)\n",
        "\n",
        "try:\n",
        "  chat_answer = chat_chain.invoke(\n",
        "      {\"question\": QUESTION},\n",
        "      config={'callbacks': [ConsoleCallbackHandler()]}\n",
        "  )\n",
        "except KeyError as e:\n",
        "  print(f\"KeyError: {e}\")\n",
        "\n",
        "# the code below is equivalent to the above but gives you less control\n",
        "# Plus as Uncle Bob once said (and I paraphrase): \"people are terrible at\n",
        "# doing things that come in pairs\", so it is likely that any of us will forget\n",
        "# to revert the debug to False. This is most useful if you want to debug every chain\n",
        "#\n",
        "# import langchain\n",
        "#\n",
        "# langchain.debug = True\n",
        "# chat_answer = chat_chain.invoke({\"input\": QUESTION})\n",
        "# langchain.debug = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqichxrc6i7t",
        "outputId": "5d5d0d9f-1bc6-4d0b-dc85-e9be7dde650e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Can you recommend me an adventure movie?\"\n",
            "}\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] [3ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3845, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] [6ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2497, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3977, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3845, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [15ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4580, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2497, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3977, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3845, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain > chain:RunnableAssign<context>] [27ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4580, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2497, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3977, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3845, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:retrieval_chain] [37ms] Chain run errored with error:\n",
            "\u001b[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2497, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 469, in invoke\\n    return self._call_with_config(self._invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3144, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 458, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 4580, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 2497, in invoke\\n    input = step.invoke(input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3977, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 1593, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\\\", line 3845, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\\\", line 380, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n\\n\\nKeyError: 'input'\"\n",
            "KeyError: 'input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_message = \"\"\"\n",
        "User question:\n",
        "{input}\n",
        "\n",
        "-----------------------------------------\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "fixed_chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_message),\n",
        "    (\"human\", human_message),\n",
        "])\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, fixed_chat_template)\n",
        "chat_chain = create_retrieval_chain(retriever, combine_docs_chain)"
      ],
      "metadata": {
        "id": "7JUWctgqZyVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "\n",
        "adventure_movies = chat_chain.invoke({\"input\": QUESTION})\n",
        "# we use pprint rather then simply print to have all the text fit the screen\n",
        "pprint(adventure_movies[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H73dm7w7rxvm",
        "outputId": "7b088d87-6cf3-48fb-c897-1c2b859316bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Ah, brave soul seeking adventure in the realm of cinema! I bestow upon thee '\n",
            " 'the recommendation of \"Raiders of the Lost Ark.\" Behold as Harrison Ford, '\n",
            " 'guided by the cinematic sorcery of Spielberg and Lucas, embarks on a daring '\n",
            " 'quest against the evilest of foes - the Nazis! With swashbuckling action, '\n",
            " 'mythical mysteries, and the power of God in a big, gold box, this Bond-style '\n",
            " 'adventure shall surely enchant thee. May your popcorn be plentiful and your '\n",
            " 'thrills be grand on this pulse-pounding cinematic escapade! ðŸŽ¥âœ¨ðŸ”¥')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as you can see this error was caused on purpose to hammer home the importance of setting:\n",
        "1. `context`: to provide the results of the retriever in `create_stuff_documents_chain`\n",
        "2. `input`: to provide the user question in  `create_retrieval_chain`"
      ],
      "metadata": {
        "id": "ppj_LD6bzFTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try with another more complicated question whose answer requires a bit more thinking. Yet after a quick inspection of each movie we can see it is indeed a surrealist movie. Oviously this answers are not deterministic and you can have different ones, but I obtained: Pan's Labyrinth, Amelie, Vertigo and Mulholland Drive."
      ],
      "metadata": {
        "id": "ctoz7xpdbc6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surrealist_movies = chat_chain.invoke({\"input\": \"Which surrealist movies should I watch ?\"})\n",
        "for key, val in surrealist_movies.items():\n",
        "  print(10 * \"-\" + f\" {key} \" + 10 * \"-\")\n",
        "  pprint(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtC850qyiBDx",
        "outputId": "e63e44ea-faae-4a95-edb8-6b08cace0fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- input ----------\n",
            "'Which surrealist movies should I watch ?'\n",
            "---------- context ----------\n",
            "[Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 61, 'name': \"Pan's Labyrinth\"}, page_content=\"Pan's Labyrinth: Guillermo Del Toro's fairy tale for grown-ups, as pull-no-punches brutal as it is gorgeously, baroquely fantastical. There's an earthy, primal feel to his fairy-world here, alien and threatening rather than gasp-inducing and 'magical', thanks in no small part to the truly cheese-dream nightmarish demon-things Del Toro conjures up, sans CGI, with the assistance of performer Doug Jones.\"),\n",
            " Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 97, 'name': 'Amelie'}, page_content=\"Amelie: Jean-Pierre Jeunet's fourth feature â€“ his second as a solo artist divorced from Marc Caro â€“ saw the\\xa0Delicatessen,\\xa0The City of Lost Children\\xa0and\\xa0Alien: Resurrection filmmaker leave behind the overwhelming darkness of his earlier works and step out into the glorious sunshine of Amelie's whimsical fantasy Paris. Sure, a cynic could read the film as the story of Audrey Tatou's monomaniacal title character's relentless, somewhat stalkerish pursuit of the hapless Nino (Matthieu Kassovitz) around Montmartre's dream-like cityscape. But this one isn't for the cynics â€” it's a tribute to the daydreamers of this world. It's a sweet, nostalgic, sentimental, beautifully sunny, and unforcedly quirky romantic comedy played out amidst a veritable visual fantasia that only Jeunet could have conceived. Amelie will always be on our list of things we like.\"),\n",
            " Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 50, 'name': 'Vertigo'}, page_content=\"Vertigo: If Psycho was Hitchcock's big shocker, then Vertigo is the one that gets properly under your skin. With James Stewart's detective stalking Kim Novak's mysterious woman, witnessing her suicide, then becoming obsessed with her double, it's certainly disturbing and most definitely (as the title suggests) disorientating. In the most artful and inventive way.\"),\n",
            " Document(metadata={'source': 'https://www.empireonline.com/movies/features/best-movies-2/', 'rank': 73, 'name': 'Mulholland Drive'}, page_content=\"Mulholland Drive: David Lynch messes with Hollywood itself in a mystery tale that's as twisted as the road it's named after, while presenting Tinseltown as both Dream Factory and a realm of Nightmares. It also put Naomi Watts on the map; her audition scene remains as stunning as it was 20 years ago.\")]\n",
            "---------- answer ----------\n",
            "('Ah, my dear curious movie enthusiast, if surrealism is what you crave, then '\n",
            " 'feast your eyes upon these cinematic marvels:\\n'\n",
            " '\\n'\n",
            " '1. \"Pan\\'s Labyrinth\" - A dark and fantastical fairy tale that will have you '\n",
            " \"questioning whether you've stepped into a dream or a nightmare. Guillermo \"\n",
            " \"Del Toro's twisted creatures and eerie visuals will leave you both enchanted \"\n",
            " 'and unsettled.\\n'\n",
            " '\\n'\n",
            " '2. \"Amelie\" - Enter the whimsical world of Jean-Pierre Jeunet\\'s Paris, '\n",
            " \"where Audrey Tautou's lovable but slightly obsessive Amelie embarks on a \"\n",
            " \"quirky journey of love and self-discovery. It's like walking through a \"\n",
            " 'pastel-colored dream filled with charm and whimsy.\\n'\n",
            " '\\n'\n",
            " '3. \"Vertigo\" - Hitchcock\\'s unsettling masterpiece that will make you '\n",
            " \"question reality and your own perceptions. James Stewart's haunting \"\n",
            " \"performance and the film's disorienting narrative will leave you feeling \"\n",
            " \"like you've stepped into a psychological whirlpool.\\n\"\n",
            " '\\n'\n",
            " '4. \"Mulholland Drive\" - David Lynch\\'s mind-bending exploration of '\n",
            " \"Hollywood's dark underbelly will dazzle and confound you. Prepare to be \"\n",
            " 'taken on a twisted journey through dreams, nightmares, and the blurred lines '\n",
            " \"between them. And remember, in Lynch's world, nothing is ever quite what it \"\n",
            " 'seems.\\n'\n",
            " '\\n'\n",
            " 'So, gather your popcorn, brace yourself for a wild ride, and immerse '\n",
            " 'yourself in the surreal wonders of these cinematic gems. Enjoy the trip down '\n",
            " 'the rabbit hole of surrealist cinema, my friend!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the 2nd question, we displayed not only the answer as previously but also the input (original question) and context (the result from the RAG). This allows us to verify that indeed that right question was fed to the model and that the context retreived ffrom our vector DB is not only relevant to the question but also that the answer is indeed rooted in the provided context and the model didn't hallucinate.\n",
        "\n",
        "This is obviously a non-scalable manual validation but it is contintutes the first step in our testing / validation of our software (ETL + database retrieval + LLM). In the next article we will see how to automatite it and scale it. Stay tuned for more"
      ],
      "metadata": {
        "id": "eTb7yrEd4M1M"
      }
    }
  ]
}